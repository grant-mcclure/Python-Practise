{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Transforms\n",
    "\n",
    "Dataset and Dataloaders help efficiently manage, load, and preprocess the data, making it possible to handle large datasets with limited memory by splitting the data into batches. Transforms allow for on-the-fly preprocessing, ensuring that data is normalized and in the right format for training the model, which leads to faster convergence. Neural networks, even in their simplest form, showcase the key steps in defining a model, training it, and evaluating it. The optimizer updates the model's weights by minimizing loss, allowing the model to improve its predictions over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, data, transform = None): #set the transform to nonde to start\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        if self.transform: #check if sample can transform\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor:\n",
    "    def __call__(self, sample):\n",
    "        features, label = sample[0], sample[1]\n",
    "        return {'features': torch.tensor(features, dtype=torch.float32),\n",
    "                'label': torch.tensor(label, dtype=torch.float32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalise:\n",
    "    def __call__(self, sample):\n",
    "        features, label = sample[0], sample[1]\n",
    "        normalised_features = (features - np.mean(features))/np.std(features)\n",
    "\n",
    "        return (normalised_features, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_data = [(np.random.rand(2), np.random.rand()) for _ in range(100)] #grab a random dataset\n",
    "\n",
    "transform = transforms.Compose([Normalise(), ToTensor()]) #compose normalise and totensor together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TabularDataset(data = tabular_data, transform = transform)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True) #chop up all the data in smaller chunks to use less memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc = nn.Linear(input_size,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN(input_size=2)\n",
    "criterion = nn.MSELoss()\n",
    "optimiser = optim.SGD(model.parameters(), lr=0.1) #adjust weigghts and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: [1/50], loss: 0.6319631093314716\n",
      "epoch: [2/50], loss: 0.09454137778707913\n",
      "epoch: [3/50], loss: 0.0973713504416602\n",
      "epoch: [4/50], loss: 0.08549400525433677\n",
      "epoch: [5/50], loss: 0.0965449448142733\n",
      "epoch: [6/50], loss: 0.08456701732107572\n",
      "epoch: [7/50], loss: 0.08976713355098452\n",
      "epoch: [8/50], loss: 0.09015543439558574\n",
      "epoch: [9/50], loss: 0.09109764812248093\n",
      "epoch: [10/50], loss: 0.09897306774343763\n",
      "epoch: [11/50], loss: 0.09502991821084704\n",
      "epoch: [12/50], loss: 0.09395231200116021\n",
      "epoch: [13/50], loss: 0.09477764368057251\n",
      "epoch: [14/50], loss: 0.09328674525022507\n",
      "epoch: [15/50], loss: 0.09275693659271513\n",
      "epoch: [16/50], loss: 0.09107466201697077\n",
      "epoch: [17/50], loss: 0.09230842334883553\n",
      "epoch: [18/50], loss: 0.08357284058417593\n",
      "epoch: [19/50], loss: 0.0810104174805539\n",
      "epoch: [20/50], loss: 0.09151668207986015\n",
      "epoch: [21/50], loss: 0.08867836743593216\n",
      "epoch: [22/50], loss: 0.0856957526079246\n",
      "epoch: [23/50], loss: 0.10875747778585979\n",
      "epoch: [24/50], loss: 0.08751359635165759\n",
      "epoch: [25/50], loss: 0.08024981657841376\n",
      "epoch: [26/50], loss: 0.09551779074328286\n",
      "epoch: [27/50], loss: 0.09831600742680686\n",
      "epoch: [28/50], loss: 0.0878548254924161\n",
      "epoch: [29/50], loss: 0.09274881812078613\n",
      "epoch: [30/50], loss: 0.09302205379520144\n",
      "epoch: [31/50], loss: 0.09507832463298525\n",
      "epoch: [32/50], loss: 0.09374487719365529\n",
      "epoch: [33/50], loss: 0.09764283150434494\n",
      "epoch: [34/50], loss: 0.0911554896405765\n",
      "epoch: [35/50], loss: 0.0915186729814325\n",
      "epoch: [36/50], loss: 0.09745645416634423\n",
      "epoch: [37/50], loss: 0.09382074752024241\n",
      "epoch: [38/50], loss: 0.09454752345170293\n",
      "epoch: [39/50], loss: 0.08976993177618299\n",
      "epoch: [40/50], loss: 0.08508883469871112\n",
      "epoch: [41/50], loss: 0.09879922973258155\n",
      "epoch: [42/50], loss: 0.08271817942815167\n",
      "epoch: [43/50], loss: 0.08758438910756793\n",
      "epoch: [44/50], loss: 0.08976224384137563\n",
      "epoch: [45/50], loss: 0.09223514688866478\n",
      "epoch: [46/50], loss: 0.0948538961155074\n",
      "epoch: [47/50], loss: 0.09556124519024577\n",
      "epoch: [48/50], loss: 0.09655018150806427\n",
      "epoch: [49/50], loss: 0.08943358649100576\n",
      "epoch: [50/50], loss: 0.09324578408684049\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        features, labels = batch['features'], batch['label']\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        outputs = model(features)\n",
    "\n",
    "        loss = criterion(outputs, labels.view(-1,1))\n",
    "\n",
    "        #run backwards pass\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss/len(dataloader)\n",
    "\n",
    "    print(f'epoch: [{epoch+1}/{num_epochs}], loss: {average_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10247238831860679\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        features, labels = batch['features'], batch['label']\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels.view(-1,1))\n",
    "        total_loss+= loss.item()\n",
    "\n",
    "    average_loss = total_loss/len(dataloader)\n",
    "\n",
    "    print(average_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "python3_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
