{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders in PyTorch\n",
    "\n",
    "Dataloaders in PyTorch are used to efficiently handle data during model training by breaking the dataset into smaller chunks, called *batches*, which helps improve both memory usage and training speed. Instead of loading the entire dataset at once, which can overwhelm memory, dataloaders load a batch at a time, making it easier to work with large datasets. They also shuffle the data automatically (if needed), preventing the model from learning any accidental patterns in the data order. Additionally, dataloaders streamline the training loop by automating tasks like batching, shuffling, and parallel data loading, making your code simpler and faster without requiring you to write custom logic for these tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Custom Dataset Class\n",
    "The `CustomDataset` class in PyTorch is a custom dataset handler that simplifies data processing for machine learning tasks. It initializes with feature data and target labels, converting them into PyTorch tensors with appropriate data types (`float32` for features and `long` for labels). The `__len__` method returns the number of samples in the dataset, which allows PyTorch's DataLoader to understand the dataset size. The `__getitem__` method retrieves individual samples based on an index, returning a dictionary containing both the feature data and corresponding label. This class facilitates easy integration with PyTorchâ€™s DataLoader for efficient data loading and batching during model training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,data,target): #essentially x and y\n",
    "        self.data = torch.tensor(data,dtype=torch.float32)\n",
    "        self.target = torch.tensor(target,dtype=torch.long)\n",
    "\n",
    "    def __len__ (self):\n",
    "        return len(self.data) #return the length of data\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = {'data': self.data[index], 'target': self.target[index]} #create a dictionary\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create instances of class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "test_dataset = CustomDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size = 32, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=train_dataset, batch_size = 32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Neural Network class and inherit from nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size,hidden_size, output_size) :\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = len(set(y_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create optimiser, criterion and instance of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        inputs = batch['data'] #same as X\n",
    "        targets = batch['target'] #similar with y\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "\n",
    "   # print(f'epoch: {epoch+1}/{num_epochs}, loss: {running_loss/len(train_dataloader)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy test:  98.817\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_predictions =[]\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        inputs = batch['data']\n",
    "        targets = batch['target']\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        predictions = torch.argmax(outputs,dim = 1)\n",
    "\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(all_targets, all_predictions)\n",
    "\n",
    "print(f'accuracy test: {accuracy*100: .3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "python3_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
